# Databricks notebook source
# MAGIC %md
# MAGIC # Document Ingestion and Preparation
# MAGIC
# MAGIC 1. Download and organize episode transcripts into a directory on DBFS
# MAGIC 2. Use LangChain to ingest those documents and split them into manageable chunks using a text splitter
# MAGIC 3. Use a sentence transformer NLP model to create embeddings of those text chunks and store them in a vectorstore
# MAGIC     * Embeddings are basically creating a high-dimension vector encoding the semantic meaning of a chunk of text

# COMMAND ----------

pip install langchain transformers mlflow[genai]>=2.9.0

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

import os
import requests
import pandas as pd
import mlflow
import mlflow.deployments

from pyspark.sql.functions import pandas_udf, explode
from pyspark.sql.types import StringType

from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE CATALOG IF NOT EXISTS leahey_sandbox;
# MAGIC USE CATALOG leahey_sandbox;
# MAGIC CREATE SCHEMA IF NOT EXISTS tgn_llm_qa;
# MAGIC USE SCHEMA tgn_llm_qa;

# COMMAND ----------

# MAGIC %md
# MAGIC ## Download episode transcripts
# MAGIC
# MAGIC Download transcripts from https://www.phfactor.net/tgn and save each in DBFS

# COMMAND ----------

urls = [f"https://www.phfactor.net/tgn/{i}.0/episode.txt" for i in range(1,5)] 
df_urls = spark.createDataFrame(urls, StringType()).toDF("url")

# COMMAND ----------

def download_transcript(url: str):
    transcript = requests.get(url, verify=False).text
    transcript.replace("\'", "'").replace('"',"").replace("\n","")
    return transcript

@pandas_udf("string")
def download_transcripts_udf(urls: pd.Series) -> pd.Series:
    return urls.apply(download_transcript)

# COMMAND ----------

tokenizer = AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer")
text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=500, chunk_overlap=50)

def split_text(text: str):
  return text_splitter.split_text(text)

@pandas_udf("array<string>")
def split_text_udf(text: pd.Series) -> pd.Series:
  return text.apply(split_text)

# COMMAND ----------

deploy_client = mlflow.deployments.get_deploy_client("databricks")

@pandas_udf("array<float>")
def get_embedding(contents: pd.Series) -> pd.Series:
    def get_embeddings(batch):
        response = deploy_client.predict(endpoint="databricks-bge-large-en", inputs={"input": batch})
        return [e["embedding"] for e in response.data]

    max_batch_size = 150
    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]

    all_embeddings = []
    for batch in batches:
        all_embeddings += get_embeddings(batch.tolist())

    return pd.Series(all_embeddings)

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS tgn_scripts (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   url STRING,
# MAGIC   content STRING,
# MAGIC   embedding ARRAY <FLOAT>
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true); 

# COMMAND ----------

(
   df_urls
    .withColumn("transcript", download_transcripts_udf("url"))
    .withColumn("content", explode(split_text_udf("transcript")))
    .withColumn("embedding", get_embedding("content"))
    .drop("transcript")
    .write.mode("overwrite").saveAsTable("tgn_scripts")
)

# COMMAND ----------

from databricks.vector_search.client import VectorSearchClient
vsc = VectorSearchClient()

if VECTOR_SEARCH_ENDPOINT_NAME not in [e['name'] for e in vsc.list_endpoints().get('endpoints', [])]:
    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type="STANDARD")

wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)
print(f"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.")
